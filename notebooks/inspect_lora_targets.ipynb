{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697c1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bc93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_linear_suffixes(model):\n",
    "    \"\"\"列出模型所有 Linear 层的去重后末级名称\"\"\"\n",
    "    suffixes = set()  # 用 set 去重\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            suffixes.add(name.split(\".\")[-1])\n",
    "    return sorted(list(suffixes))  # 排序方便查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ad6919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qwen3 = AutoModel.from_pretrained(\"../../llms/Qwen/Qwen3-0.6B\")\n",
    "print(list_linear_suffixes(qwen3))\n",
    "print(qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc86f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
      "MiniCPMModel(\n",
      "  (embed_tokens): Embedding(73448, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x MiniCPMDecoderLayer(\n",
      "      (self_attn): MiniCPMFlashAttention2(\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=128, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=128, bias=False)\n",
      "        (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (rotary_emb): MiniCPMLongRoPE()\n",
      "      )\n",
      "      (mlp): MiniCPMMLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): MiniCPMRMSNorm()\n",
      "      (post_attention_layernorm): MiniCPMRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MiniCPMRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "minicpm4 = AutoModel.from_pretrained(\"../../llms/openbmb/MiniCPM4-0.5B\", trust_remote_code=True)\n",
    "print(list_linear_suffixes(minicpm4))\n",
    "print(minicpm4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d9e9ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense', 'key', 'query', 'value']\n",
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 1024)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-23): 24 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSdpaSelfAttention(\n",
      "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): XLMRobertaPooler(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mle5 = AutoModel.from_pretrained(\"../../llms/intfloat/multilingual-e5-large\", trust_remote_code=True)\n",
    "print(list_linear_suffixes(mle5))\n",
    "print(mle5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859ec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
